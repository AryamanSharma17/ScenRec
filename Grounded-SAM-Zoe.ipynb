{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO AND TUTORIAL FOR LES FURNITURES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dirmain = os.getcwd()\n",
    "print(dirmain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AryamanSharma17/Grounded-Segment-Anything"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Meta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirmain = dir + '/Grounded-Segment-Anything'\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything')\n",
    "%pip install -q -r requirements.txt\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything/segment_anything')\n",
    "%pip install -q .\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything/GroundingDINO')\n",
    "!pip install -q .\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New directory should be: Main Directory + Grounded-Segment-Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounding DINO\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file) \n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    \n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location=device)\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "\n",
    "\n",
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device='cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dirmain+'/Grounded-Segment-Anything/sam_vit_b_01ec64.pth')==False:\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "sam_checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = dirmain +'/Resource/classroom_simple.jpg'\n",
    "image_source, image = load_image(test_image)\n",
    "source_image = Image.fromarray(image_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = source_image.resize((500,500))\n",
    "image_source = np.asarray(source_image)\n",
    "Image.fromarray(image_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box and center generation with GDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image, text_prompt, model, box_threshold = 0.4, text_threshold = 0.2):\n",
    "  boxes, logits, phrases = predict(\n",
    "      model=model, \n",
    "      image=image, \n",
    "      caption=text_prompt,\n",
    "      box_threshold=box_threshold,\n",
    "      text_threshold=text_threshold\n",
    "  )\n",
    "\n",
    "  annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "  annotated_frame = annotated_frame[...,::-1] # BGR to RGB \n",
    "  return annotated_frame, boxes,phrases, logits\n",
    "\n",
    "\n",
    "def gen_mask_Gd(image_source, boxes):\n",
    "    h,w,_ = image_source.shape\n",
    "    boxes_unnorm = boxes * torch.Tensor([w,h,w,h]) \n",
    "    boxes_xyxy = box_convert(boxes =boxes_unnorm, in_fmt = \"cxcywh\", out_fmt = \"xyxy\").numpy()\n",
    "    mask = np.zeros_like(image_source)\n",
    "    for box in boxes_xyxy:\n",
    "        x0, y0,x1,y1 = box\n",
    "        mask[int(y0):int(y1), int(x0):int(x1),:] = 255\n",
    "    return mask, boxes_xyxy, boxes_unnorm\n",
    "\n",
    "def center(centerx,centery,detected_boxes):\n",
    "    for i in range(len(detected_boxes)):\n",
    "        centerx.append(int(detected_boxes[i][0] + detected_boxes[i][2])/2)\n",
    "        centery.append(int(detected_boxes[i][1] + detected_boxes[i][3])/2)\n",
    "    return centerx,centery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_frame, detected_boxes,phrases, logits = detect(image, text_prompt=\"chair. table. sofa. door. window\", model=groundingdino_model, box_threshold=0.5 )\n",
    "anoim = Image.fromarray(annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask,box_change,box_unnormalized = gen_mask_Gd(image_source,detected_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerx = []\n",
    "centery = []\n",
    "\n",
    "centerx,centery = center(centerx,centery,box_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "c = np.array([\"red\",\"green\",\"yellow\",\"cyan\",\"orange\", \"lime\", \"pink\",\"Maroon\", \"purple\", \"aquamarine\", \"olive\", \"khakhi\"])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(centerx,centery )\n",
    "plt.imshow(anoim)\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAM: Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cuda()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.cpu().reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Mask Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
    "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Mask fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = annotated_frame\n",
    "for i in range(len(segmented_frame_masks)):\n",
    "  all_frames = draw_mask(segmented_frame_masks[i][0], all_frames)\n",
    "Image.fromarray(all_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "for i in range(1,len(segmented_frame_masks)):\n",
    "    allmask += segmented_frame_masks[i][0].cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmask_image = Image.fromarray(allmask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(allmask_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the image to invert the axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allmask_image=allmask_image.rotate(180)\n",
    "y_main = np.where(allmask==1)[0]\n",
    "x_main = np.where(allmask==1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_all_mask = np.array(allmask_image, dtype=np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EROSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((4,4),np.uint8)\n",
    "erosion = cv2.erode(int_all_mask, kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_erode = np.where(erosion==1)[0]\n",
    "x_erode = np.where(erosion==1)[1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eroded Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_erode,y_erode, 8)\n",
    "plt.gca().invert_xaxis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_main,y_main, 8)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.zeros(len(phrases))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating output file for VR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.empty((len(phrases),9),dtype=object)\n",
    "data.fill([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    for j in range(4):\n",
    "        data[i][j] = (box_change[i][j])\n",
    "    data[i][4] = centerx[i]\n",
    "    data[i][5] = centery[i]\n",
    "    data[i][6] = depth[i]\n",
    "    data[i][7] = (phrases[i])\n",
    "    data[i][8] = logits[i].item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectangles : ABCD\n",
    "\n",
    "A = 0, 1\n",
    "\n",
    "B = 0, 3\n",
    "\n",
    "C = 2, 3\n",
    "\n",
    "D = 2, 1\n",
    "\n",
    "center = 4, 5\n",
    "\n",
    "depth centner = 6\n",
    "\n",
    "furniture type = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns=['Coord 1','Coord 2','Coord 3','Coord 4','Center_X', 'Center_Y', 'Depth', 'Furniture category', 'Confidence'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dirmain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving to the end after depth estimation \n",
    "# df.to_csv('Vr.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPTH ESTIMATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/isl-org/ZoeDepth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ZoeDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python sanity.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from zoedepth.utils.misc import get_image_from_url, colorize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "zoe = torch.hub.load(\".\", \"ZoeD_N\", source=\"local\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe = zoe.to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = Image.open(\"test.png\")\n",
    "img = Image.open(test_image)\n",
    "img.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth = zoe.infer_pil(img)\n",
    "\n",
    "\n",
    "colored_depth = colorize(depth)\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,7))\n",
    "for ax, im, title in zip(axs, [img, colored_depth], ['Input', 'Predicted Depth']):\n",
    "  ax.imshow(im)\n",
    "  ax.axis('off')\n",
    "  ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dirmain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_x = df[\"Center_X\"]\n",
    "center_y = df[\"Center_Y\"]\n",
    "print(len(df))\n",
    "for i in range(0, len(df)):\n",
    "    depth_val = depth[int(center_x[i]), int(center_y[i])]\n",
    "    df.at[i, 'Depth'] = depth_val\n",
    "\n",
    "print(\"depth max is:\" + str(depth.max()) + \",depth min is:\" + str(depth.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vr.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
