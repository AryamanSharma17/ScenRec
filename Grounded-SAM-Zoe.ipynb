{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO AND TUTORIAL FOR LES FURNITURES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dirmain = os.getcwd()\n",
    "print(dirmain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AryamanSharma17/Grounded-Segment-Anything#!pip install --upgrade timm\n",
    "!git clone https://github.com/isl-org/ZoeDepth.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirmain = dir + '/Grounded-Segment-Anything'\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything')\n",
    "%pip install -q -r requirements.txt\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything/segment_anything')\n",
    "%pip install -q .\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything/GroundingDINO')\n",
    "!pip install -q .\n",
    "os.chdir(dirmain + '/Grounded-Segment-Anything')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New directory should be: Main Directory + Grounded-Segment-Anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grounding DINO\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "\n",
    "#ZoeDepth\n",
    "from ZoeDepth.zoedepth.utils.misc import get_image_from_url, colorize\n",
    "%pip install timm==0.6.7\n",
    "\n",
    "#Hugging Face\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file) \n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    \n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location=device)\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "\n",
    "\n",
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device='cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dirmain+'/Grounded-Segment-Anything/sam_vit_b_01ec64.pth')==False:\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "sam_checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe = torch.hub.load(\".\", \"ZoeD_N\", source=\"local\", pretrained=True)\n",
    "zoe = zoe.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box and center generation with GDINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image, text_prompt, model, box_threshold = 0.4, text_threshold = 0.2):\n",
    "  boxes, logits, phrases = predict(\n",
    "      model=model, \n",
    "      image=image, \n",
    "      caption=text_prompt,\n",
    "      box_threshold=box_threshold,\n",
    "      text_threshold=text_threshold\n",
    "  )\n",
    "\n",
    "  annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "  annotated_frame = annotated_frame[...,::-1] # BGR to RGB \n",
    "  return annotated_frame, boxes,phrases, logits\n",
    "\n",
    "\n",
    "def gen_mask_Gd(image_source, boxes):\n",
    "    h,w,_ = image_source.shape\n",
    "    boxes_unnorm = boxes * torch.Tensor([w,h,w,h]) \n",
    "    boxes_xyxy = box_convert(boxes =boxes_unnorm, in_fmt = \"cxcywh\", out_fmt = \"xyxy\").numpy()\n",
    "    mask = np.zeros_like(image_source)\n",
    "    for box in boxes_xyxy:\n",
    "        x0, y0,x1,y1 = box\n",
    "        mask[int(y0):int(y1), int(x0):int(x1),:] = 255\n",
    "    return mask, boxes_xyxy, boxes_unnorm\n",
    "\n",
    "def center(centerx,centery,detected_boxes):\n",
    "    for i in range(len(detected_boxes)):\n",
    "        centerx.append(int(detected_boxes[i][0] + detected_boxes[i][2])/2)\n",
    "        centery.append(int(detected_boxes[i][1] + detected_boxes[i][3])/2)\n",
    "    return centerx,centery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image, sam_model, boxes):\n",
    "  sam_model.set_image(image)\n",
    "  H, W, _ = image.shape\n",
    "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
    "  masks, _, _ = sam_model.predict_torch(\n",
    "      point_coords = None,\n",
    "      point_labels = None,\n",
    "      boxes = transformed_boxes,\n",
    "      multimask_output = False,\n",
    "      )\n",
    "  return masks.cuda()\n",
    "  \n",
    "\n",
    "def draw_mask(mask, image, random_color=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.cpu().reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = dirmain +'/Resource/classroom_simple.jpg'\n",
    "image_source, image = load_image(test_image)\n",
    "source_image = Image.fromarray(image_source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = source_image.resize((int(source_image.width/5),int(source_image.height/5)))image_source = np.asarray(source_image)\n",
    "Image.fromarray(image_source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECT DETECTION AND CLASSIFICATION "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding box and class label generation with Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_frame, detected_boxes,phrases, logits = detect(image, text_prompt=\"chair. table. sofa. door. window\", model=groundingdino_model, box_threshold=0.5 )\n",
    "anoim = Image.fromarray(annotated_frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box Normalixation and snap to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask,box_change,box_unnormalized = gen_mask_Gd(image_source,detected_boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center Calculation for each bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerx = []\n",
    "centery = []\n",
    "\n",
    "centerx,centery = center(centerx,centery,box_change)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "c = np.array([\"red\",\"green\",\"yellow\",\"cyan\",\"orange\", \"lime\", \"pink\",\"Maroon\", \"purple\", \"aquamarine\", \"olive\", \"khakhi\"])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(centerx,centery )\n",
    "plt.imshow(anoim)\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Segmentation with SAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
    "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Mask fusion and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = annotated_frame\n",
    "for i in range(len(segmented_frame_masks)):\n",
    "  all_frames = draw_mask(segmented_frame_masks[i][0], all_frames)\n",
    "Image.fromarray(all_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion into BW Mask image segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "for i in range(1,len(segmented_frame_masks)):\n",
    "    allmask += segmented_frame_masks[i][0].cpu().numpy() \n",
    "allmask_image = Image.fromarray(allmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_all_mask = np.array(allmask_image, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list_main = np.empty((len(phrases)),dtype=object)\n",
    "mask_list_main.fill([])\n",
    "for i in range(len(mask_list_main)):\n",
    "    # mask_list_main[i][0] = np.where(==1)[0]\n",
    "    # mask_list_main[i][1] = np.where(image_list[i]==1)[1]\n",
    "    mask_list_main[i] =(segmented_frame_masks[i][0].cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all into UINT8 type and perform mask wise erosion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EROSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_main = np.where(allmask==1)[0]\n",
    "x_main = np.where(allmask==1)[1]\n",
    "kernel = np.ones((2,2),np.uint8)\n",
    "erosion_all = cv2.erode(int_all_mask, kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list_erosion = np.empty((len(phrases),2),dtype=object)\n",
    "mask_list_erosion.fill([])\n",
    "for i in range(len(mask_list_erosion)):\n",
    "    mask_list_erosion[i][0] = cv2.erode(mask_list_main[i], kernel, iterations=1)\n",
    "    mask_list_erosion[i][1] = cv2.erode(mask_list_main[i], kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_erode = np.where(erosion_all==1)[0]\n",
    "x_erode = np.where(erosion_all==1)[1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eroded Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_erode,y_erode, 8)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_main,y_main, 8)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dirmain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Estimation with ZOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = zoe.infer_pil(source_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_depth = colorize(depth)\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,7))\n",
    "for ax, im, title in zip(axs, [source_image, colored_depth], ['Input', 'Predicted Depth']):\n",
    "  ax.imshow(im)\n",
    "  ax.axis('off')\n",
    "  ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_val = np.empty(len(phrases))\n",
    "for i in range(len(phrases)):\n",
    "    sum = 0\n",
    "    depth_temp = 0\n",
    "    buff = (np.where(mask_list_main[i] == True))\n",
    "    for j in range(len(mask_list_main[i])):\n",
    "        sum += depth[buff[0][j], buff[1][j]]\n",
    "    depth_temp=sum/len(buff[0])\n",
    "    print(i, depth_temp)\n",
    "    depth_val[i] = depth_temp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating output file for VR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.empty((len(phrases),9),dtype=object)\n",
    "data.fill([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fur = {'chair': 0, 'table': 1, 'sofa': 2, 'door': 3, 'window': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    for j in range(4):\n",
    "        data[i][j] = (box_change[i][j])\n",
    "    data[i][4] = centerx[i]\n",
    "    data[i][5] = centery[i]\n",
    "    data[i][6] = depth_val[i]\n",
    "    data[i][7] = dict_fur[(phrases[i])]\n",
    "    data[i][8] = logits[i].item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectangles : ABCD\n",
    "\n",
    "A = 0, 1\n",
    "\n",
    "B = 0, 3\n",
    "\n",
    "C = 2, 3\n",
    "\n",
    "D = 2, 1\n",
    "\n",
    "center = 4, 5\n",
    "\n",
    "depth center = 6\n",
    "\n",
    "furniture type = 7 (`chair`: 0, `table`: 1, `sofa`: 2, `door`: 3, `window`: 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns=['Coord_1','Coord_2','Coord_3','Coord_4','Center_X', 'Center_Y', 'Depth', 'Furniture_category', 'Confidence'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(r'Vr_Zoe.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
